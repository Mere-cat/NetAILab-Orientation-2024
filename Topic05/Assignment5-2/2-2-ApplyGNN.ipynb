{"cells":[{"cell_type":"markdown","id":"8f4a91d8","metadata":{"id":"8f4a91d8"},"source":["# Applying GNN Models\n","\n","In this lecture, we will continue using the Cora example from the previous lesson. You will learn about:\n","\n","- Unsupervised GRL\n","- GNNs for Supervised Downstream Tasks\n","\n","We will also compare these methods with the approach discussed in the previous lesson."]},{"cell_type":"code","execution_count":1,"id":"fb43d373","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25952,"status":"ok","timestamp":1663384864744,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"fb43d373","outputId":"836e9449-86ab-48cd-d998-5fd0622d55a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.4.1\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","# Comment below while running on server\n","# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","# !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n","\n","import os.path as osp\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.linear_model import LogisticRegression\n","from torch_geometric.loader import LinkNeighborLoader"]},{"cell_type":"markdown","id":"a9e4160e","metadata":{},"source":["## Unsupervised Graph Representation Learning with GraphSAGE\n","Since we aim to learn graph representations through an unsupervised method, we do not use node labels for training.<br>\n","\n","We assume that if there is a link between a pair of nodes, those nodes should have similar embeddings. Conversely, if there is no link between a pair of nodes, their embeddings should be dissimilar.\n","\n","Based on this assumption, we can define the following loss function:\n"," \n","\\begin{equation}\n","\\text{Loss} = - \\left( \\log \\left( \\sigma(h_u^{\\top} h_v) \\right) - \\sum_{i=1}^k \\log \\left( \\sigma(h_u^{\\top} h_{n_i}) \\right) \\right) , n_i \\sim P_V\n","\\end{equation}\n","\n","- $\\log \\left( \\sigma (h_u^{\\top} h_v) \\right)$:  The similarity between the positive sample pair (i.e., true neighbors). Maximizing this term means you want the similarity of positive samples to be as high as possible.\n","  \n","- $- \\sum_{i=1}^k \\log \\left( \\sigma (h_u^{\\top} h_{n_i}) \\right)$: The similarity between the negative sample pairs (i.e., non-neighbors). Minimizing this term means you want the similarity of negative samples to be as low as possible.\n","\n","Once the embeddings are obtained, they are fed into an additional classifier for node classification."]},{"cell_type":"code","execution_count":2,"id":"YbBjup3U9MQk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3020,"status":"ok","timestamp":1663386340276,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"YbBjup3U9MQk","outputId":"465f6186-36da-486b-c75f-aceb8876cb54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","dataset = 'Cora'\n","path = osp.join('.', 'data', dataset)\n","dataset = Planetoid(root=path, name='Cora', transform=NormalizeFeatures())\n","data = dataset[0]\n","print(data)"]},{"cell_type":"code","execution_count":3,"id":"e0c194b0","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386341904,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e0c194b0"},"outputs":[],"source":["from torch_geometric.nn import SAGEConv\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, num_layers):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.convs = nn.ModuleList()\n","        for i in range(num_layers):\n","            in_channels = in_channels if i == 0 else hidden_channels\n","            self.convs.append(SAGEConv(in_channels, hidden_channels))\n","\n","    def forward(self, x, edge_index):\n","        for i, conv in enumerate(self.convs):\n","            x = conv(x, edge_index)\n","            if i != self.num_layers - 1:\n","                x = F.relu(x)\n","                x = F.dropout(x, p=0.5, training=self.training)\n","        return x"]},{"cell_type":"code","execution_count":4,"id":"583902a9","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386459760,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"583902a9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/merecat/miniconda3/envs/PyG/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n","  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"]}],"source":["# define neighbor sampler 抽一些邊\n","train_loader = LinkNeighborLoader(\n","    data,\n","    batch_size=256,\n","    shuffle=True,\n","    neg_sampling_ratio=1.0, #沒編數量和有邊一樣\n","    num_neighbors=[10, 10],\n",")"]},{"cell_type":"code","execution_count":5,"id":"BK7x8-jo9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663386460858,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BK7x8-jo9MQl","outputId":"823cdf0f-de02-47c1-8696-70e42e26b13c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2422, 1433], edge_index=[2, 7975], y=[2422], train_mask=[2422], val_mask=[2422], test_mask=[2422], n_id=[2422], e_id=[7975], input_id=[256], edge_label_index=[2, 512], edge_label=[512])\n"]}],"source":["for batch in train_loader:\n","    print(batch)\n","    break"]},{"cell_type":"code","execution_count":6,"id":"kWzD1vPb9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1663386488587,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"kWzD1vPb9MQl","outputId":"ae18e461-89fa-43c3-c0af-16f1d6d005a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Edge label index: containing both positive and negative edges\n","tensor([[264, 641, 766,  ..., 204, 759, 287],\n","        [821, 637,  81,  ..., 103, 690, 461]])\n","Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.])\n"]}],"source":["print(\"Edge label index: containing both positive and negative edges\")\n","print(batch.edge_label_index)\n","\n","print(\"Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\")\n","print(batch.edge_label)"]},{"cell_type":"code","execution_count":7,"id":"b5890c80","metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GraphSAGE(data.num_node_features, hidden_channels=64, num_layers=2)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n","model = model.to(device)\n","x, edge_index = data.x.to(device), data.edge_index.to(device)"]},{"cell_type":"code","execution_count":8,"id":"BaT6bJq89MQl","metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1663386518319,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BaT6bJq89MQl"},"outputs":[],"source":["# define training and testing functions\n","def train():\n","    model.train()\n","\n","    total_loss = 0\n","    for batch in train_loader:\n","        batch = batch.to(device)\n","        optimizer.zero_grad()\n","        embedding = model(batch.x, batch.edge_index)\n","        embedding_src = embedding[batch.edge_label_index[0]]\n","        embedding_dst = embedding[batch.edge_label_index[1]]\n","        pred = (embedding_src * embedding_dst).sum(dim=-1)\n","        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += float(loss) * pred.size(0)\n","\n","    return total_loss / data.num_nodes\n","\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    out = model(data.x.to(device), data.edge_index.to(device)).cpu() \n","\n","    clf = LogisticRegression()\n","    clf.fit(out[data.train_mask], data.y[data.train_mask])\n","\n","    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n","    test_acc = clf.score(out[data.test_mask], data.y[data.test_mask])\n","\n","    return val_acc, test_acc"]},{"cell_type":"code","execution_count":9,"id":"e88903af","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78288,"status":"ok","timestamp":1663386603993,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e88903af","outputId":"a9284f85-5802-437a-eb7c-c4cba8e390ac","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 5.1435, Val: 0.3040, Test: 0.3220\n","Epoch: 002, Loss: 4.5006, Val: 0.4940, Test: 0.4830\n","Epoch: 003, Loss: 4.3884, Val: 0.6100, Test: 0.5980\n","Epoch: 004, Loss: 4.2570, Val: 0.6220, Test: 0.6020\n","Epoch: 005, Loss: 4.0741, Val: 0.6440, Test: 0.6150\n","Epoch: 006, Loss: 4.0370, Val: 0.6580, Test: 0.6380\n","Epoch: 007, Loss: 3.9373, Val: 0.6860, Test: 0.6490\n","Epoch: 008, Loss: 3.8935, Val: 0.6740, Test: 0.6840\n","Epoch: 009, Loss: 3.8764, Val: 0.6820, Test: 0.6970\n","Epoch: 010, Loss: 3.8944, Val: 0.7000, Test: 0.7110\n","Epoch: 011, Loss: 3.8216, Val: 0.7180, Test: 0.7290\n","Epoch: 012, Loss: 3.7838, Val: 0.7220, Test: 0.7350\n","Epoch: 013, Loss: 3.7782, Val: 0.7200, Test: 0.7280\n","Epoch: 014, Loss: 3.7867, Val: 0.7260, Test: 0.7440\n","Epoch: 015, Loss: 3.7502, Val: 0.7160, Test: 0.7570\n","Epoch: 016, Loss: 3.7116, Val: 0.7200, Test: 0.7610\n","Epoch: 017, Loss: 3.6975, Val: 0.7460, Test: 0.7720\n","Epoch: 018, Loss: 3.7230, Val: 0.7520, Test: 0.7780\n","Epoch: 019, Loss: 3.7097, Val: 0.7320, Test: 0.7700\n","Epoch: 020, Loss: 3.6901, Val: 0.7200, Test: 0.7700\n","Epoch: 021, Loss: 3.6929, Val: 0.7280, Test: 0.7650\n","Epoch: 022, Loss: 3.6716, Val: 0.7160, Test: 0.7610\n","Epoch: 023, Loss: 3.6975, Val: 0.7240, Test: 0.7580\n","Epoch: 024, Loss: 3.6324, Val: 0.7360, Test: 0.7800\n","Epoch: 025, Loss: 3.6566, Val: 0.7540, Test: 0.7870\n","Epoch: 026, Loss: 3.6217, Val: 0.7560, Test: 0.7820\n","Epoch: 027, Loss: 3.6296, Val: 0.7520, Test: 0.7930\n","Epoch: 028, Loss: 3.6178, Val: 0.7520, Test: 0.8020\n","Epoch: 029, Loss: 3.6258, Val: 0.7620, Test: 0.7950\n","Epoch: 030, Loss: 3.6464, Val: 0.7760, Test: 0.7930\n","Epoch: 031, Loss: 3.6178, Val: 0.7620, Test: 0.7890\n","Epoch: 032, Loss: 3.6173, Val: 0.7560, Test: 0.7820\n","Epoch: 033, Loss: 3.5891, Val: 0.7640, Test: 0.7740\n","Epoch: 034, Loss: 3.5859, Val: 0.7620, Test: 0.7760\n","Epoch: 035, Loss: 3.5896, Val: 0.7760, Test: 0.7800\n","Epoch: 036, Loss: 3.5865, Val: 0.7860, Test: 0.7930\n","Epoch: 037, Loss: 3.5946, Val: 0.7900, Test: 0.7910\n","Epoch: 038, Loss: 3.5650, Val: 0.7900, Test: 0.7990\n","Epoch: 039, Loss: 3.6077, Val: 0.7840, Test: 0.7930\n","Epoch: 040, Loss: 3.6017, Val: 0.7780, Test: 0.7810\n","Epoch: 041, Loss: 3.5667, Val: 0.7700, Test: 0.7800\n","Epoch: 042, Loss: 3.5670, Val: 0.7620, Test: 0.7790\n","Epoch: 043, Loss: 3.5312, Val: 0.7620, Test: 0.7700\n","Epoch: 044, Loss: 3.5504, Val: 0.7520, Test: 0.7600\n","Epoch: 045, Loss: 3.5800, Val: 0.7700, Test: 0.7730\n","Epoch: 046, Loss: 3.5551, Val: 0.7740, Test: 0.7690\n","Epoch: 047, Loss: 3.5697, Val: 0.7680, Test: 0.7720\n","Epoch: 048, Loss: 3.5364, Val: 0.7580, Test: 0.7810\n","Epoch: 049, Loss: 3.5582, Val: 0.7580, Test: 0.7800\n","Epoch: 050, Loss: 3.5434, Val: 0.7480, Test: 0.7870\n","Epoch: 051, Loss: 3.5650, Val: 0.7440, Test: 0.7850\n","Epoch: 052, Loss: 3.5670, Val: 0.7340, Test: 0.7850\n","Epoch: 053, Loss: 3.5247, Val: 0.7340, Test: 0.7870\n","Epoch: 054, Loss: 3.5053, Val: 0.7440, Test: 0.7780\n","Epoch: 055, Loss: 3.5583, Val: 0.7480, Test: 0.7840\n","Epoch: 056, Loss: 3.5049, Val: 0.7500, Test: 0.7830\n","Epoch: 057, Loss: 3.5468, Val: 0.7520, Test: 0.7890\n","Epoch: 058, Loss: 3.5605, Val: 0.7300, Test: 0.7700\n","Epoch: 059, Loss: 3.5121, Val: 0.7340, Test: 0.7680\n","Epoch: 060, Loss: 3.5353, Val: 0.7480, Test: 0.7720\n","Epoch: 061, Loss: 3.5238, Val: 0.7540, Test: 0.7760\n","Epoch: 062, Loss: 3.5696, Val: 0.7400, Test: 0.7710\n","Epoch: 063, Loss: 3.4808, Val: 0.7360, Test: 0.7870\n","Epoch: 064, Loss: 3.5199, Val: 0.7400, Test: 0.7790\n","Epoch: 065, Loss: 3.5402, Val: 0.7420, Test: 0.7770\n","Epoch: 066, Loss: 3.4630, Val: 0.7400, Test: 0.7830\n","Epoch: 067, Loss: 3.5409, Val: 0.7480, Test: 0.7720\n","Epoch: 068, Loss: 3.5511, Val: 0.7520, Test: 0.7740\n","Epoch: 069, Loss: 3.4708, Val: 0.7580, Test: 0.7710\n","Epoch: 070, Loss: 3.5047, Val: 0.7500, Test: 0.7700\n","Epoch: 071, Loss: 3.5146, Val: 0.7560, Test: 0.7820\n","Epoch: 072, Loss: 3.5194, Val: 0.7540, Test: 0.7820\n","Epoch: 073, Loss: 3.5427, Val: 0.7600, Test: 0.7820\n","Epoch: 074, Loss: 3.5175, Val: 0.7580, Test: 0.7730\n","Epoch: 075, Loss: 3.5178, Val: 0.7620, Test: 0.7810\n","Epoch: 076, Loss: 3.5130, Val: 0.7700, Test: 0.7880\n","Epoch: 077, Loss: 3.4900, Val: 0.7820, Test: 0.7970\n","Epoch: 078, Loss: 3.4817, Val: 0.7740, Test: 0.7930\n","Epoch: 079, Loss: 3.5167, Val: 0.7760, Test: 0.7860\n","Epoch: 080, Loss: 3.4882, Val: 0.7760, Test: 0.7950\n","Epoch: 081, Loss: 3.4622, Val: 0.7680, Test: 0.7920\n","Epoch: 082, Loss: 3.5077, Val: 0.7740, Test: 0.7970\n","Epoch: 083, Loss: 3.5106, Val: 0.7680, Test: 0.7890\n","Epoch: 084, Loss: 3.5073, Val: 0.7660, Test: 0.7980\n","Epoch: 085, Loss: 3.4769, Val: 0.7700, Test: 0.8010\n","Epoch: 086, Loss: 3.4740, Val: 0.7720, Test: 0.7950\n","Epoch: 087, Loss: 3.4964, Val: 0.7800, Test: 0.7960\n","Epoch: 088, Loss: 3.4797, Val: 0.7680, Test: 0.7960\n","Epoch: 089, Loss: 3.4696, Val: 0.7700, Test: 0.7920\n","Epoch: 090, Loss: 3.4873, Val: 0.7760, Test: 0.7880\n","Epoch: 091, Loss: 3.5098, Val: 0.7800, Test: 0.7940\n","Epoch: 092, Loss: 3.5324, Val: 0.7720, Test: 0.7980\n","Epoch: 093, Loss: 3.4690, Val: 0.7840, Test: 0.7950\n","Epoch: 094, Loss: 3.5031, Val: 0.7660, Test: 0.8030\n","Epoch: 095, Loss: 3.5316, Val: 0.7760, Test: 0.7940\n","Epoch: 096, Loss: 3.5097, Val: 0.7780, Test: 0.7960\n","Epoch: 097, Loss: 3.5040, Val: 0.7820, Test: 0.7890\n","Epoch: 098, Loss: 3.4786, Val: 0.7700, Test: 0.7910\n","Epoch: 099, Loss: 3.4517, Val: 0.7620, Test: 0.7900\n","Epoch: 100, Loss: 3.4906, Val: 0.7600, Test: 0.7950\n"]}],"source":["for epoch in range(1, 101):\n","    loss = train()\n","    val_acc, test_acc = test()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n","          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"]},{"cell_type":"markdown","id":"6caee53d","metadata":{"id":"6caee53d"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","\n","We make use of node feature and graph structure at the same time and boost the accuracy up to **0.791** with a simple two-layer `GraphSAGE`."]},{"cell_type":"markdown","id":"36240400","metadata":{"id":"36240400"},"source":["## End-to-end semi-supervised learning with Graph Convolution Network(GCN)\n","Previously, we adopt a two stage classification pipeline where we first extract network feature via unsupervised learning then utilize a classifier to predict its label. <br>\n","The two-stage design could be suboptimal since the network features were not extracted for specific task. <br>\n","Therefore, we now employ an end-to-end approach. This ensures that the features learned by the model are directly aligned with the specific task, potentially leading to better performance."]},{"cell_type":"code","execution_count":10,"id":"9b156ae0","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1663386768615,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"9b156ae0"},"outputs":[],"source":["from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True,\n","                             normalize=True)\n","        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True,\n","                             normalize=True)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":11,"id":"67338214","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386768902,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"67338214","outputId":"07232e0d-90e6-43ef-e805-859283b424b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(1433, 64)\n","  (conv2): GCNConv(64, 7)\n",")\n"]}],"source":["dim = 64\n","model = GCN(dataset.num_features, dim, dataset.num_classes)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-4)\n","print(model)"]},{"cell_type":"code","execution_count":12,"id":"646fb96c","metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1663386794576,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"646fb96c"},"outputs":[],"source":["def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data.x, data.edge_index, data.edge_weight)\n","    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    return float(loss)\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    pred = model(data.x, data.edge_index, data.edge_weight).argmax(dim=-1)\n","    mask = data.test_mask\n","    accs = (int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","    return accs"]},{"cell_type":"code","execution_count":13,"id":"142e2595","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2101,"status":"ok","timestamp":1663386797988,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"142e2595","outputId":"9596e72b-144a-43f7-ba6f-078dcc2338e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss:1.9460 Testing accuracy:0.1810\n","Loss:1.9440 Testing accuracy:0.2800\n","Loss:1.9427 Testing accuracy:0.3220\n","Loss:1.9408 Testing accuracy:0.3690\n","Loss:1.9394 Testing accuracy:0.4310\n","Loss:1.9375 Testing accuracy:0.5010\n","Loss:1.9352 Testing accuracy:0.5610\n","Loss:1.9334 Testing accuracy:0.6070\n","Loss:1.9310 Testing accuracy:0.6440\n","Loss:1.9286 Testing accuracy:0.6620\n","Loss:1.9266 Testing accuracy:0.6790\n","Loss:1.9244 Testing accuracy:0.6820\n","Loss:1.9208 Testing accuracy:0.6920\n","Loss:1.9190 Testing accuracy:0.6920\n","Loss:1.9169 Testing accuracy:0.6930\n","Loss:1.9125 Testing accuracy:0.6990\n","Loss:1.9119 Testing accuracy:0.7110\n","Loss:1.9082 Testing accuracy:0.7160\n","Loss:1.9056 Testing accuracy:0.7270\n","Loss:1.9011 Testing accuracy:0.7300\n","Loss:1.8995 Testing accuracy:0.7420\n","Loss:1.8966 Testing accuracy:0.7480\n","Loss:1.8938 Testing accuracy:0.7570\n","Loss:1.8892 Testing accuracy:0.7620\n","Loss:1.8863 Testing accuracy:0.7630\n","Loss:1.8828 Testing accuracy:0.7640\n","Loss:1.8794 Testing accuracy:0.7670\n","Loss:1.8757 Testing accuracy:0.7670\n","Loss:1.8739 Testing accuracy:0.7690\n","Loss:1.8673 Testing accuracy:0.7710\n","Loss:1.8640 Testing accuracy:0.7700\n","Loss:1.8625 Testing accuracy:0.7720\n","Loss:1.8592 Testing accuracy:0.7740\n","Loss:1.8546 Testing accuracy:0.7770\n","Loss:1.8514 Testing accuracy:0.7790\n","Loss:1.8493 Testing accuracy:0.7770\n","Loss:1.8460 Testing accuracy:0.7780\n","Loss:1.8410 Testing accuracy:0.7770\n","Loss:1.8402 Testing accuracy:0.7790\n","Loss:1.8302 Testing accuracy:0.7790\n","Loss:1.8269 Testing accuracy:0.7790\n","Loss:1.8232 Testing accuracy:0.7770\n","Loss:1.8202 Testing accuracy:0.7770\n","Loss:1.8177 Testing accuracy:0.7780\n","Loss:1.8102 Testing accuracy:0.7780\n","Loss:1.8065 Testing accuracy:0.7800\n","Loss:1.8045 Testing accuracy:0.7810\n","Loss:1.7942 Testing accuracy:0.7800\n","Loss:1.7933 Testing accuracy:0.7800\n","Loss:1.7875 Testing accuracy:0.7820\n","Loss:1.7905 Testing accuracy:0.7830\n","Loss:1.7797 Testing accuracy:0.7840\n","Loss:1.7771 Testing accuracy:0.7840\n","Loss:1.7644 Testing accuracy:0.7850\n","Loss:1.7624 Testing accuracy:0.7860\n","Loss:1.7603 Testing accuracy:0.7860\n","Loss:1.7575 Testing accuracy:0.7880\n","Loss:1.7523 Testing accuracy:0.7890\n","Loss:1.7492 Testing accuracy:0.7890\n","Loss:1.7424 Testing accuracy:0.7900\n","Loss:1.7348 Testing accuracy:0.7900\n","Loss:1.7305 Testing accuracy:0.7920\n","Loss:1.7229 Testing accuracy:0.7910\n","Loss:1.7176 Testing accuracy:0.7900\n","Loss:1.7196 Testing accuracy:0.7900\n","Loss:1.7154 Testing accuracy:0.7900\n","Loss:1.7037 Testing accuracy:0.7900\n","Loss:1.6978 Testing accuracy:0.7870\n","Loss:1.6892 Testing accuracy:0.7870\n","Loss:1.6902 Testing accuracy:0.7870\n","Loss:1.6870 Testing accuracy:0.7860\n","Loss:1.6766 Testing accuracy:0.7860\n","Loss:1.6637 Testing accuracy:0.7860\n","Loss:1.6657 Testing accuracy:0.7870\n","Loss:1.6621 Testing accuracy:0.7880\n","Loss:1.6538 Testing accuracy:0.7880\n","Loss:1.6461 Testing accuracy:0.7880\n","Loss:1.6477 Testing accuracy:0.7860\n","Loss:1.6417 Testing accuracy:0.7860\n","Loss:1.6355 Testing accuracy:0.7860\n","Loss:1.6258 Testing accuracy:0.7860\n","Loss:1.6161 Testing accuracy:0.7870\n","Loss:1.6093 Testing accuracy:0.7870\n","Loss:1.6066 Testing accuracy:0.7870\n","Loss:1.5972 Testing accuracy:0.7850\n","Loss:1.5898 Testing accuracy:0.7850\n","Loss:1.5833 Testing accuracy:0.7850\n","Loss:1.5848 Testing accuracy:0.7860\n","Loss:1.5826 Testing accuracy:0.7880\n","Loss:1.5646 Testing accuracy:0.7880\n","Loss:1.5647 Testing accuracy:0.7880\n","Loss:1.5614 Testing accuracy:0.7890\n","Loss:1.5481 Testing accuracy:0.7900\n","Loss:1.5428 Testing accuracy:0.7910\n","Loss:1.5276 Testing accuracy:0.7910\n","Loss:1.5259 Testing accuracy:0.7910\n","Loss:1.5337 Testing accuracy:0.7910\n","Loss:1.5157 Testing accuracy:0.7890\n","Loss:1.5072 Testing accuracy:0.7880\n","Loss:1.5131 Testing accuracy:0.7890\n","Loss:1.4943 Testing accuracy:0.7900\n","Loss:1.4921 Testing accuracy:0.7900\n","Loss:1.4862 Testing accuracy:0.7900\n","Loss:1.4652 Testing accuracy:0.7900\n","Loss:1.4811 Testing accuracy:0.7920\n","Loss:1.4612 Testing accuracy:0.7930\n","Loss:1.4509 Testing accuracy:0.7920\n","Loss:1.4525 Testing accuracy:0.7940\n","Loss:1.4539 Testing accuracy:0.7920\n","Loss:1.4412 Testing accuracy:0.7930\n","Loss:1.4353 Testing accuracy:0.7940\n","Loss:1.4247 Testing accuracy:0.7930\n","Loss:1.4107 Testing accuracy:0.7940\n","Loss:1.4117 Testing accuracy:0.7930\n","Loss:1.3963 Testing accuracy:0.7940\n","Loss:1.3857 Testing accuracy:0.7950\n","Loss:1.3900 Testing accuracy:0.7940\n","Loss:1.3825 Testing accuracy:0.7930\n","Loss:1.3815 Testing accuracy:0.7930\n","Loss:1.3721 Testing accuracy:0.7930\n","Loss:1.3468 Testing accuracy:0.7940\n","Loss:1.3563 Testing accuracy:0.7940\n","Loss:1.3474 Testing accuracy:0.7940\n","Loss:1.3460 Testing accuracy:0.7930\n","Loss:1.3267 Testing accuracy:0.7940\n","Loss:1.3346 Testing accuracy:0.7930\n","Loss:1.3135 Testing accuracy:0.7940\n","Loss:1.3263 Testing accuracy:0.7950\n","Loss:1.3073 Testing accuracy:0.7950\n","Loss:1.2972 Testing accuracy:0.7950\n","Loss:1.2959 Testing accuracy:0.7950\n","Loss:1.2916 Testing accuracy:0.7950\n","Loss:1.2756 Testing accuracy:0.7950\n","Loss:1.2874 Testing accuracy:0.7950\n","Loss:1.2612 Testing accuracy:0.7960\n","Loss:1.2463 Testing accuracy:0.7970\n","Loss:1.2391 Testing accuracy:0.7970\n","Loss:1.2419 Testing accuracy:0.7970\n","Loss:1.2310 Testing accuracy:0.7970\n","Loss:1.2113 Testing accuracy:0.7980\n","Loss:1.2251 Testing accuracy:0.7980\n","Loss:1.2253 Testing accuracy:0.7970\n","Loss:1.2111 Testing accuracy:0.7970\n","Loss:1.1851 Testing accuracy:0.7960\n","Loss:1.2001 Testing accuracy:0.7960\n","Loss:1.1754 Testing accuracy:0.7960\n","Loss:1.1798 Testing accuracy:0.7960\n","Loss:1.1427 Testing accuracy:0.7960\n","Loss:1.1727 Testing accuracy:0.7960\n","Loss:1.1530 Testing accuracy:0.7940\n","Loss:1.1486 Testing accuracy:0.7950\n","Loss:1.1421 Testing accuracy:0.7950\n","Loss:1.1416 Testing accuracy:0.7950\n","Loss:1.1325 Testing accuracy:0.7940\n","Loss:1.1117 Testing accuracy:0.7960\n","Loss:1.1200 Testing accuracy:0.7960\n","Loss:1.1012 Testing accuracy:0.7960\n","Loss:1.0965 Testing accuracy:0.7950\n","Loss:1.0959 Testing accuracy:0.7930\n","Loss:1.0840 Testing accuracy:0.7940\n","Loss:1.0955 Testing accuracy:0.7950\n","Loss:1.0700 Testing accuracy:0.7970\n","Loss:1.0484 Testing accuracy:0.7970\n","Loss:1.0555 Testing accuracy:0.7970\n","Loss:1.0484 Testing accuracy:0.7960\n","Loss:1.0542 Testing accuracy:0.7960\n","Loss:1.0391 Testing accuracy:0.7970\n","Loss:1.0348 Testing accuracy:0.7970\n","Loss:1.0404 Testing accuracy:0.7980\n","Loss:1.0154 Testing accuracy:0.7980\n","Loss:0.9965 Testing accuracy:0.7980\n","Loss:1.0197 Testing accuracy:0.7970\n","Loss:1.0024 Testing accuracy:0.7980\n","Loss:0.9843 Testing accuracy:0.7990\n","Loss:0.9945 Testing accuracy:0.8000\n","Loss:0.9838 Testing accuracy:0.8000\n","Loss:0.9933 Testing accuracy:0.8010\n","Loss:0.9726 Testing accuracy:0.8000\n","Loss:0.9418 Testing accuracy:0.8000\n","Loss:0.9466 Testing accuracy:0.8010\n","Loss:0.9420 Testing accuracy:0.8000\n","Loss:0.9189 Testing accuracy:0.8000\n","Loss:0.9505 Testing accuracy:0.8000\n","Loss:0.9379 Testing accuracy:0.8000\n","Loss:0.9183 Testing accuracy:0.7990\n","Loss:0.9412 Testing accuracy:0.8000\n","Loss:0.9023 Testing accuracy:0.8020\n","Loss:0.9067 Testing accuracy:0.8020\n","Loss:0.9089 Testing accuracy:0.8010\n","Loss:0.9005 Testing accuracy:0.8020\n","Loss:0.8977 Testing accuracy:0.8030\n","Loss:0.8712 Testing accuracy:0.8030\n","Loss:0.8679 Testing accuracy:0.8030\n","Loss:0.8777 Testing accuracy:0.8030\n","Loss:0.8551 Testing accuracy:0.8040\n","Loss:0.8575 Testing accuracy:0.8070\n","Loss:0.8615 Testing accuracy:0.8070\n","Loss:0.8443 Testing accuracy:0.8070\n","Loss:0.8473 Testing accuracy:0.8080\n","Loss:0.8297 Testing accuracy:0.8070\n"]}],"source":["for epoch in range(200):\n","    loss = train()\n","    test_acc = test()\n","    print(f\"Loss:{loss:.4f} Testing accuracy:{test_acc:.4f}\")"]},{"cell_type":"markdown","id":"ec8ff377","metadata":{"id":"ec8ff377"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","4. GCN: end-to-end learning with `Accuracy:0.818`\n","\n","From this example, we clearly figure out that using GCN with end2end training leads to the best performance since the feature extracted could be learned to optimize the node classification task."]},{"cell_type":"markdown","id":"d347b13c","metadata":{"id":"d347b13c"},"source":["## Applying different GNN backbone layer\n","The full list of implemented GNN could be found in [here.](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)"]},{"cell_type":"code","execution_count":14,"id":"765f7d34","metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1663387016492,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"765f7d34"},"outputs":[],"source":["from torch_geometric.nn import GCNConv,GATConv, GraphSAGE\n","import torch.nn.functional as F\n","\n","class GNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, gnn_type):\n","        super().__init__()\n","        if gnn_type == \"GCN\":\n","            self.GNN = GCNConv\n","        elif gnn_type == \"SAGE\":\n","            self.GNN = GraphSAGE\n","        elif gnn_type == \"GAT\":\n","            self.GNN = GATConv\n","        \n","        self.conv1 = self.GNN(in_channels, hidden_channels)\n","        self.conv2 = self.GNN(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":15,"id":"f37bd755","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663387016912,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f37bd755","outputId":"7c33b9d5-4dd0-4305-ca6e-c1bee2a074d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["GNN(\n","  (conv1): GATConv(1433, 32, heads=1)\n","  (conv2): GATConv(32, 7, heads=1)\n",")\n"]}],"source":["dim = 32\n","gnn_type = \"GAT\"\n","model = GNN(dataset.num_features, dim, dataset.num_classes,gnn_type=gnn_type)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","print(model)"]},{"cell_type":"markdown","id":"afe00d31","metadata":{"id":"afe00d31"},"source":["# HW: Link prediction with GNN\n","1. Try different GNN layer\n","2. Try to optimize the performance by stacking multiple layers\n","3. Report the best accuracy on testing set and the best model configuration(e.g., how many layers?)"]},{"cell_type":"code","execution_count":16,"id":"cfb91924","metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1663387050805,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"cfb91924"},"outputs":[],"source":["# Let's practice how to use GNN for link prediction\n","# First we need to load the Cora dataset\n","\n","from sklearn.metrics import roc_auc_score\n","import torch_geometric.transforms as T\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.utils import negative_sampling\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","transform = T.Compose([\n","    T.NormalizeFeatures(),\n","    T.ToDevice(device),\n","    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n","                      add_negative_train_samples=True),\n","])\n","dataset = Planetoid(path, name='Cora', transform=transform)\n","train_data, val_data, test_data = dataset[0]"]},{"cell_type":"code","execution_count":17,"id":"43bed8c6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1663387060011,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"43bed8c6","outputId":"24659c22-d453-4534-db45-9d742e94c9b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------Training data------\n","Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[8976], edge_label_index=[2, 8976])\n","Training edges:\n","tensor([[ 403,  300,  112,  ..., 1685, 1573, 1623],\n","        [1742,  634, 1623,  ..., 1592, 2671, 1795]], device='cuda:0')\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')\n","\n","--------Testing data------\n","Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n","Testing edges:\n","tensor([[ 577,  694,  673,  ...,  672,  454, 2198],\n","        [1518, 1478, 1907,  ..., 2023, 2620, 2022]], device='cuda:0')\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')\n"]}],"source":["print(\"--------Training data------\")\n","print(train_data)\n","print(\"Training edges:\")\n","print(train_data.edge_label_index)\n","print(\"Labels\")\n","print(train_data.edge_label)\n","\n","print()\n","print(\"--------Testing data------\")\n","print(test_data)\n","print(\"Testing edges:\")\n","print(test_data.edge_label_index)\n","print(\"Labels\")\n","print(test_data.edge_label)"]},{"cell_type":"code","execution_count":18,"id":"8ccc3487","metadata":{"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1663389253599,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8ccc3487"},"outputs":[],"source":["class MyGNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        ############################################################################\n","        # TODO: Your code here! \n","        # create you GNN layer here. \n","        # try to use different GNN backbone layer or stacking multiple layer to boost performance\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, out_channels)\n","        \n","        self.dropout = torch.nn.Dropout(p=0.5)\n","        ############################################################################\n","\n","    def forward(self, x, edge_index):\n","        ############################################################################\n","        # TODO: Your code here! \n","        # Apply the forward pass according to your GNN layers\n","        # you shoud return the embedding of each node (x has shape [num_nodes, dim])    \n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = self.dropout(x)  # Dropout to regularize\n","\n","        x = self.conv2(x, edge_index)\n","        ############################################################################\n","        return x\n","    \n","    def get_prediction(self, node_embedding, edges):\n","        # In this function, we have the node embedding and edges as input\n","        # Input shapes:\n","        #      node_embedding: (|V|, out_channels)\n","        #      edges: (2, number of edges)\n","        # To generate such output, we use the inner product of embeddings of two nodes\n","        # The output is to generate a scalar for each pair of edge\n","        embedding_first_node = node_embedding[edges[0]]\n","        embedding_second_node = node_embedding[edges[1]]\n","        ############################################################################\n","        # TODO: Your code here! \n","        # implement the element-wise product as edge feature for link prediction\n","        inner_product = torch.sum(embedding_first_node * embedding_second_node, dim=-1)\n","        \n","        ############################################################################\n","        return inner_product"]},{"cell_type":"code","execution_count":19,"id":"8236b1d0","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254662,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8236b1d0"},"outputs":[],"source":["############################################################################\n","# TODO: Your code here! \n","# initiate your GNN model and select the criterion for link prediction\n","\n","model = MyGNN(dataset.num_features, 128, 64).to(device)\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n","# criterion = torch.nn.BCEWithLogitsLoss()\n","criterion = torch.nn.MSELoss()\n","############################################################################"]},{"cell_type":"code","execution_count":20,"id":"52305879","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"52305879"},"outputs":[],"source":["# Implement the train function\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    embedding = model(train_data.x, train_data.edge_index)\n","\n","    # We perform a new round of negative sampling for every training epoch:\n","    neg_edge_index = negative_sampling(\n","        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n","        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n","\n","    edge_label_index = torch.cat(\n","        [train_data.edge_label_index, neg_edge_index],\n","        dim=-1,\n","    )\n","    \n","    # Please assign the target for negative edges\n","    edge_label = torch.cat([\n","        train_data.edge_label,\n","        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n","    ], dim=0)\n","    \n","    # make prediction\n","    prediction = model.get_prediction(embedding, edge_label_index).view(-1)\n","    \n","    # optimization\n","    loss = criterion(prediction, edge_label)\n","    loss.backward()\n","    optimizer.step()\n","    return loss"]},{"cell_type":"code","execution_count":21,"id":"ad8d6158","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"ad8d6158"},"outputs":[],"source":["# Implement the test function\n","@torch.no_grad()\n","def test(data):\n","    model.eval()\n","    embedding = model(data.x, data.edge_index)\n","    \n","    # use the sigmoid function to normalize our prediction into [0,1]\n","    out = model.get_prediction(embedding, data.edge_label_index).view(-1).sigmoid()\n","    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"]},{"cell_type":"code","execution_count":22,"id":"f60a1f15","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2360,"status":"ok","timestamp":1663389257017,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f60a1f15","outputId":"c3f96ee5-852a-4357-950d-add431c7cd23","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 0.2434, Val: 0.6765, Test: 0.6541\n","Epoch: 002, Loss: 0.1603, Val: 0.6877, Test: 0.6560\n","Epoch: 003, Loss: 0.1613, Val: 0.6902, Test: 0.6544\n","Epoch: 004, Loss: 0.1834, Val: 0.6968, Test: 0.6562\n","Epoch: 005, Loss: 0.1643, Val: 0.7017, Test: 0.6578\n","Epoch: 006, Loss: 0.1805, Val: 0.7017, Test: 0.6580\n","Epoch: 007, Loss: 0.1745, Val: 0.7001, Test: 0.6573\n","Epoch: 008, Loss: 0.1567, Val: 0.6986, Test: 0.6569\n","Epoch: 009, Loss: 0.1682, Val: 0.6995, Test: 0.6572\n","Epoch: 010, Loss: 0.1631, Val: 0.7012, Test: 0.6582\n","Epoch: 011, Loss: 0.1540, Val: 0.7025, Test: 0.6600\n","Epoch: 012, Loss: 0.1596, Val: 0.7037, Test: 0.6615\n","Epoch: 013, Loss: 0.1623, Val: 0.7031, Test: 0.6625\n","Epoch: 014, Loss: 0.1574, Val: 0.7019, Test: 0.6632\n","Epoch: 015, Loss: 0.1524, Val: 0.7011, Test: 0.6640\n","Epoch: 016, Loss: 0.1545, Val: 0.7011, Test: 0.6656\n","Epoch: 017, Loss: 0.1542, Val: 0.7029, Test: 0.6691\n","Epoch: 018, Loss: 0.1503, Val: 0.7054, Test: 0.6749\n","Epoch: 019, Loss: 0.1499, Val: 0.7083, Test: 0.6813\n","Epoch: 020, Loss: 0.1510, Val: 0.7111, Test: 0.6862\n","Epoch: 021, Loss: 0.1490, Val: 0.7143, Test: 0.6882\n","Epoch: 022, Loss: 0.1453, Val: 0.7175, Test: 0.6899\n","Epoch: 023, Loss: 0.1448, Val: 0.7235, Test: 0.6978\n","Epoch: 024, Loss: 0.1435, Val: 0.7357, Test: 0.7162\n","Epoch: 025, Loss: 0.1375, Val: 0.7557, Test: 0.7442\n","Epoch: 026, Loss: 0.1370, Val: 0.7683, Test: 0.7607\n","Epoch: 027, Loss: 0.1317, Val: 0.7777, Test: 0.7706\n","Epoch: 028, Loss: 0.1266, Val: 0.7926, Test: 0.7849\n","Epoch: 029, Loss: 0.1261, Val: 0.8101, Test: 0.8058\n","Epoch: 030, Loss: 0.1202, Val: 0.8238, Test: 0.8253\n","Epoch: 031, Loss: 0.1159, Val: 0.8333, Test: 0.8384\n","Epoch: 032, Loss: 0.1169, Val: 0.8339, Test: 0.8417\n","Epoch: 033, Loss: 0.1115, Val: 0.8370, Test: 0.8434\n","Epoch: 034, Loss: 0.1100, Val: 0.8464, Test: 0.8518\n","Epoch: 035, Loss: 0.1069, Val: 0.8590, Test: 0.8644\n","Epoch: 036, Loss: 0.1047, Val: 0.8651, Test: 0.8689\n","Epoch: 037, Loss: 0.1025, Val: 0.8656, Test: 0.8625\n","Epoch: 038, Loss: 0.0984, Val: 0.8603, Test: 0.8574\n","Epoch: 039, Loss: 0.0988, Val: 0.8615, Test: 0.8622\n","Epoch: 040, Loss: 0.0970, Val: 0.8675, Test: 0.8707\n","Epoch: 041, Loss: 0.0949, Val: 0.8694, Test: 0.8742\n","Epoch: 042, Loss: 0.0924, Val: 0.8681, Test: 0.8722\n","Epoch: 043, Loss: 0.0900, Val: 0.8688, Test: 0.8726\n","Epoch: 044, Loss: 0.0895, Val: 0.8779, Test: 0.8806\n","Epoch: 045, Loss: 0.0852, Val: 0.8845, Test: 0.8875\n","Epoch: 046, Loss: 0.0850, Val: 0.8859, Test: 0.8904\n","Epoch: 047, Loss: 0.0834, Val: 0.8850, Test: 0.8922\n","Epoch: 048, Loss: 0.0838, Val: 0.8867, Test: 0.8951\n","Epoch: 049, Loss: 0.0817, Val: 0.8888, Test: 0.8979\n","Epoch: 050, Loss: 0.0781, Val: 0.8908, Test: 0.8998\n","Epoch: 051, Loss: 0.0809, Val: 0.8879, Test: 0.9005\n","Epoch: 052, Loss: 0.0774, Val: 0.8851, Test: 0.8978\n","Epoch: 053, Loss: 0.0755, Val: 0.8896, Test: 0.8999\n","Epoch: 054, Loss: 0.0748, Val: 0.8937, Test: 0.9011\n","Epoch: 055, Loss: 0.0755, Val: 0.8949, Test: 0.9014\n","Epoch: 056, Loss: 0.0729, Val: 0.8973, Test: 0.9048\n","Epoch: 057, Loss: 0.0732, Val: 0.8967, Test: 0.9046\n","Epoch: 058, Loss: 0.0712, Val: 0.8956, Test: 0.9028\n","Epoch: 059, Loss: 0.0716, Val: 0.8998, Test: 0.9065\n","Epoch: 060, Loss: 0.0695, Val: 0.9040, Test: 0.9097\n","Epoch: 061, Loss: 0.0692, Val: 0.9050, Test: 0.9091\n","Epoch: 062, Loss: 0.0694, Val: 0.9048, Test: 0.9079\n","Epoch: 063, Loss: 0.0694, Val: 0.9073, Test: 0.9090\n","Epoch: 064, Loss: 0.0668, Val: 0.9107, Test: 0.9122\n","Epoch: 065, Loss: 0.0657, Val: 0.9114, Test: 0.9117\n","Epoch: 066, Loss: 0.0669, Val: 0.9093, Test: 0.9099\n","Epoch: 067, Loss: 0.0657, Val: 0.9110, Test: 0.9105\n","Epoch: 068, Loss: 0.0642, Val: 0.9119, Test: 0.9097\n","Epoch: 069, Loss: 0.0621, Val: 0.9132, Test: 0.9091\n","Epoch: 070, Loss: 0.0638, Val: 0.9150, Test: 0.9100\n","Epoch: 071, Loss: 0.0621, Val: 0.9162, Test: 0.9109\n","Epoch: 072, Loss: 0.0607, Val: 0.9172, Test: 0.9121\n","Epoch: 073, Loss: 0.0593, Val: 0.9192, Test: 0.9141\n","Epoch: 074, Loss: 0.0596, Val: 0.9191, Test: 0.9149\n","Epoch: 075, Loss: 0.0614, Val: 0.9195, Test: 0.9152\n","Epoch: 076, Loss: 0.0595, Val: 0.9194, Test: 0.9159\n","Epoch: 077, Loss: 0.0591, Val: 0.9210, Test: 0.9179\n","Epoch: 078, Loss: 0.0577, Val: 0.9218, Test: 0.9184\n","Epoch: 079, Loss: 0.0568, Val: 0.9210, Test: 0.9168\n","Epoch: 080, Loss: 0.0588, Val: 0.9205, Test: 0.9146\n","Epoch: 081, Loss: 0.0579, Val: 0.9226, Test: 0.9154\n","Epoch: 082, Loss: 0.0564, Val: 0.9216, Test: 0.9142\n","Epoch: 083, Loss: 0.0566, Val: 0.9226, Test: 0.9147\n","Epoch: 084, Loss: 0.0556, Val: 0.9252, Test: 0.9164\n","Epoch: 085, Loss: 0.0559, Val: 0.9260, Test: 0.9172\n","Epoch: 086, Loss: 0.0544, Val: 0.9254, Test: 0.9186\n","Epoch: 087, Loss: 0.0538, Val: 0.9247, Test: 0.9189\n","Epoch: 088, Loss: 0.0551, Val: 0.9249, Test: 0.9192\n","Epoch: 089, Loss: 0.0537, Val: 0.9264, Test: 0.9187\n","Epoch: 090, Loss: 0.0532, Val: 0.9276, Test: 0.9180\n","Epoch: 091, Loss: 0.0525, Val: 0.9271, Test: 0.9167\n","Epoch: 092, Loss: 0.0528, Val: 0.9270, Test: 0.9162\n","Epoch: 093, Loss: 0.0530, Val: 0.9272, Test: 0.9162\n","Epoch: 094, Loss: 0.0522, Val: 0.9285, Test: 0.9178\n","Epoch: 095, Loss: 0.0519, Val: 0.9288, Test: 0.9183\n","Epoch: 096, Loss: 0.0510, Val: 0.9286, Test: 0.9187\n","Epoch: 097, Loss: 0.0513, Val: 0.9279, Test: 0.9185\n","Epoch: 098, Loss: 0.0521, Val: 0.9280, Test: 0.9180\n","Epoch: 099, Loss: 0.0511, Val: 0.9281, Test: 0.9178\n","Epoch: 100, Loss: 0.0504, Val: 0.9295, Test: 0.9184\n","Final Test: 0.9184\n"]}],"source":["best_val_auc = final_test_auc = 0\n","for epoch in range(1, 101):\n","    loss = train()\n","    val_auc = test(val_data)\n","    test_auc = test(test_data)\n","    if val_auc > best_val_auc:\n","        best_val = val_auc\n","        final_test_auc = test_auc\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n","          f'Test: {test_auc:.4f}')\n","\n","print(f'Final Test: {final_test_auc:.4f}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
